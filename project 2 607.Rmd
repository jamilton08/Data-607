---
title: "Project 2 607"
author: "Jonathan Cruz"
date: "2024-02-29"
output: html_document
---
# Rank Schools's Routes Difficulty
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(hrbrthemes)
```

## Using API to load data
NYC data allows you to collect data make a url request and loads it as a csv

```{r}
bus <- read.csv("https://data.cityofnewyork.us/resource/ez4e-fazm.csv?$offset=150&$limit=653000")
```
## Limit Dataset
We want to limit data exclusively to traffic issues as we are only concerned about delays that occured because of traffic and it's also important to note that about 62% of bus delays are due to traffic as it's shown below.

```{r}
full_table_row = nrow(bus)
bus <- bus |> filter(reason == "Heavy Traffic")
traffic_row = nrow(bus)

traffic_delay_percent = traffic_row / full_table_row * 100

print(traffic_delay_percent)
```
## Preview of Initial Data
Here we can see that we have "mins", "minutes" capitals, missing values, incorrect values  and ranges of numbers and on some ocasions even hours are listed so obviously this data is not clean enough to run an analysis
```{r}
bus |> 
  select(how_long_delayed) |>
  head(20)
```

## Clean "how_long_delayed" column
We must clean in a way were we keep the integrity of the data and save as much rows as possible without compromising values that can be truly extracted. For ranges of minutes we will average them out since the middle of those ranges wont change the integrity of how long a delay was since it was between two times making any value in between true. Hours will be converted into minutes and empty, exagerated values or values that we can't denote a logical sense of it will be removed.
```{r}

bus<- bus |> 
    mutate(how_long_delayed = str_remove_all(how_long_delayed, " ")) |>
    mutate(how_long_delayed = str_remove_all(how_long_delayed, "\\.")) |>
    mutate(time_stuff = tolower(str_extract(how_long_delayed, "[A-Za-z]+" ))) |>
    mutate(how_long_delayed = str_remove(how_long_delayed, "[A-Za-z]+")) |>
    mutate(is_range = str_detect(how_long_delayed, "-|/"))|> 
    mutate(end_range = ifelse(is_range,str_extract(how_long_delayed, "\\d+$"), 0)) |> 
    mutate(how_long_delayed = ifelse(is_range,str_remove(how_long_delayed, "-\\d+$|/\\d+$"), how_long_delayed))

bus <- transform(bus, how_long_delayed = as.integer(how_long_delayed))

bus <- transform(bus, end_range = as.integer(end_range))

bus <- bus |> 
    mutate(how_long_delayed = ifelse(str_detect(time_stuff, "hr|hrs|hours|hour|h"), how_long_delayed * 60, how_long_delayed * 1))

bus <- bus |> 
    drop_na(how_long_delayed) |>
    filter( how_long_delayed < 300) |>
    mutate(average_hours = ifelse(is_range, (how_long_delayed + end_range) / 2 , how_long_delayed))

filtered_row <- nrow(bus)

bus |> 
  select(how_long_delayed) |>
  head(20)
#bus <- bus |> mutate(how_long_delayed = str_extract(how_long_delayed, "\\d+"))
```

## Get loss data percent
We managed  to save about 97.5 percent of the heavy traffic dataset

```{r}
percent_loss = (1 - (filtered_row / traffic_row) ) * 100
print(percent_loss)
```
## Serviced Schools
We run into issues were serviced schools on one row are seperated by commas, dots or forward slash. Some of the id numbers start with hashtags.
```{r}

bus |> 
  select(schools_serviced) |>
  head(20)
```

## Expand rows and Clean Serviced Schools
Schools have a 5 digit identification number and anything over it or under it will not be used for  analyisis since a human error can damage the integrity of the data and what it means. It does not occur much often but it's better to neglect data we can understand rather than input false data to a specific school
```{r}
bus <- bus |> mutate(schools_serviced = str_remove_all(schools_serviced, "#|\\(|\\)"))|> mutate(schools_serviced = str_remove(schools_serviced, "^ | $"))
sep <- bus |> separate_longer_delim(schools_serviced, delim=',') |> separate_longer_delim(schools_serviced, delim='.') |> separate_longer_delim(schools_serviced, delim=' ') |> filter(str_detect(schools_serviced, "^.....$"))

sep |> 
  select(schools_serviced) |>
  head(20)
```

## Analysis on school with highest delay
we found that busses delay averages and count of occurences tend to normally distributed(Q-Q plots have a line that closely matches a diagnal line) so we will create and index in which 85 % of it's strength depends on occurences and 15% of its strengh depends on average seeing where each z score lies and they're comparable to each other accordingly based on their ranges
```{r,  fig.show="hold", out.width="50%"}
delay_rank <- sep |> group_by(schools_serviced) |> summarise(average = mean(how_long_delayed), n = n())

delay_mean = mean(delay_rank$average)
delay_sd = sd(delay_rank$average)
count_mean = mean(delay_rank$n)
count_sd = sd(delay_rank$n)
ggplot(delay_rank, aes(x = average)) + geom_histogram() + labs(title = "Delay averages")

sim_norm <- rnorm(n = nrow(delay_rank), mean = delay_mean, sd = delay_sd)
ggplot(data = delay_rank, aes(sample = sim_norm)) + geom_line(stat = "qq")

sim_norm_c <- rnorm(n = nrow(delay_rank), mean = count_mean, sd = count_sd)
ggplot(data = delay_rank, aes(sample = sim_norm_c)) + geom_line(stat = "qq")
```

## Evaluate Ranks and Generate Difficulty Score
average is the average minutes delayed per occurence(n). We use the probability percentage of each observation is "average" and "n" values to determine how much of the distribution it occupies. The more of the distribution it occupies, the higher the score. X amounts of standard deveation means higher score and x amount of standard deveation to the negative side means lower standard deveation.
```{r}

delay_rank <- delay_rank |> mutate(count_score = pnorm(q = n, mean = count_mean, sd = count_sd), delay_score = pnorm(q = average, mean = delay_mean, sd = delay_sd)) |> mutate(difficulty_score = (count_score * 0.85) + (delay_score * 0.15))


 
 delay_rank|> 
  head(20)
```

## Plot top 50 most difficult schools
Since we managed to succesfully provide a diffulty score of schools routes, we will now plot the top 50 schools with the most difficult routes.
```{r fig1, fig.height = 20, fig.width = 10}
delay_rank |> 
    arrange(desc(difficulty_score)) |>
    slice(1:50)|>
    ggplot( aes(x=reorder(schools_serviced, +difficulty_score), y = difficulty_score, fill = average))  + geom_bar(stat="identity", width = 0.8) + coord_flip() + theme_ipsum() + geom_text(
     aes(label = format(round(difficulty_score * 100, 2), nsmall = 1)),
     colour = "white", size = 4.4,
     vjust =0.45,hjust = 1.2, position = position_dodge(0.1)
 ) + labs(title = "Top 50 Schools With Most Difficult Route", y= "Bus Routes Difficulty Percentage", x = "Schools", fill = "Average Delay(minutes)") + theme(axis.title = element_text(family = "Helvetica", size = (20), colour = "steelblue4", face = "bold.italic"), plot.title =  element_text(family = "Helvetica", size = (20), colour = "steelblue4", face = "bold.italic"), plot.subtitle =  element_text(family = "Helvetica", size = (12), colour = "lightblue3", face = "bold.italic"))
```
## Conclusion And Where Do We Go From Here

In conclusion, we now have a way to determine which schools has tougher routes than others, furthermore we can inspect through districts and boroughs using the school numbers to match them, and by knowing this information, the district and borough leaders and decide where to allocate funding in order 
